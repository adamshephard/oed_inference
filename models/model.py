import numpy as np
import torch
from torch import nn
import torch.nn.functional as F  # noqa: N812
from tiatoolbox.models.models_abc import ModelABC
from tiatoolbox.utils import misc
from collections import OrderedDict

from .transunet.vit_seg_model import VisionTransformer as ViT_seg
from .transunet.vit_seg_model import CONFIGS as CONFIGS_ViT_seg
from .utils import convert_pytorch_checkpoint, crop_op


class TransUNet(ModelABC):
    def __init__(self: ModelABC,
                 weights=None,
                 nr_layers=None,
                 img_size=512,
                 patch_size=16
        ) -> None:
        super().__init__()
        self.nr_layers = nr_layers
        encoder_backbone_name = "R50-ViT-B_16"
        config_vit = CONFIGS_ViT_seg[encoder_backbone_name]
        config_vit.n_classes = nr_layers
        config_vit.n_skip = 3
        if encoder_backbone_name.find('R50') != -1:
            config_vit.patches.grid = (int(img_size / patch_size), int(img_size / patch_size))
        self.model = ViT_seg(config_vit, img_size=img_size, num_classes=nr_layers, freeze=False).cuda()
        # self.model.load_from(weights=np.load(f'/data/ANTICIPATE/dyplasia_detection/dysplasia_detector/pretrained/ViT_model_weights/imagenet21k/{encoder_backbone_name}.npz'))
        if weights is not None:
            saved_state_dict = torch.load(weights)["desc"]
            saved_state_dict = convert_pytorch_checkpoint(saved_state_dict)
            saved_state_dict2 = saved_state_dict.copy()
            for k, v in saved_state_dict.items():
                if k.startswith('model.'):
                    saved_state_dict2.pop(k)
                    saved_state_dict2[k[6:]] = v
            self.model.load_state_dict(saved_state_dict2, strict=True)#False) 
                                
    def forward(self: ModelABC, img_list: list) -> torch.nn.Module:
        """Model forward function."""
        # must be rgb input
        imgs = img_list / 255.0  # to 0-1 range
        out = self.model(imgs)
        # out = crop_op(out, [92, 92])
        # out = crop_op(out[0], [184, 184])
        out = crop_op(out, [184, 184])
        out_dict = OrderedDict()
        out_dict['ls'] = out
        return out_dict

    @staticmethod
    # skipcq: PYL-W0221  # noqa: ERA001
    def postproc(raw_maps: np.ndarray) -> np.ndarray:
        """Post-processing script for image tiles.

        Args:
            raw_maps (:class:`numpy.ndarray`):
                Prediction output

        Returns:
            :class:`numpy.ndarray` - Layer map:
                Pixel-wise nuclear layer segmentation prediction.

        """
        ls_map = raw_maps
        return ls_map
    
    @staticmethod
    def infer_batch(model: nn.Module, batch_data: np.ndarray, on_gpu: bool) -> np.ndarray:
    # def infer_batch(model: nn.Module, batch_data: np.ndarray, *, on_gpu: bool) -> np.ndarray:
        """Run inference on an input batch.

        This contains logic for forward operation as well as batch i/o
        aggregation.

        Args:
            model (nn.Module):
                PyTorch defined model.
            batch_data (ndarray):
                A batch of data generated by
                `torch.utils.data.DataLoader`.
            on_gpu (bool):
                Whether to run inference on a GPU.

        Returns:
            tuple:
                Output from each head. Each head is expected to contain
                N predictions for N input patches.

        """
        patch_imgs = batch_data

        device = misc.select_device(on_gpu=on_gpu)
        patch_imgs_gpu = patch_imgs.to(device).type(torch.float32)  # to NCHW
        patch_imgs_gpu = patch_imgs_gpu.permute(0, 3, 1, 2).contiguous()

        model.eval()  # infer mode

        # --------------------------------------------------------------
        with torch.inference_mode():
            pred_dict = model(patch_imgs_gpu)
            pred_dict = OrderedDict(
                [[k, v.permute(0, 2, 3, 1).contiguous()] for k, v in pred_dict.items()],
            )
            layer_map = F.softmax(pred_dict["ls"], dim=-1)
            # keep pixel level values
        return [layer_map.cpu().numpy()]
        #     # take max value
        #     layer_map = torch.argmax(layer_map, dim=-1)#, keepdim=True)
        #     pred_dict["ls"] = layer_map
        #     pred_output = pred_dict["ls"]
        # return [pred_output.cpu().numpy().astype("uint8")]

